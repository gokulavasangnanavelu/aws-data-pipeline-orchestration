{
	"jobConfig": {
		"name": "glue-encryption-job-1",
		"description": "",
		"role": "arn:aws:iam::655214492809:role/glue-IAM-Execution-role",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 2,
		"maxCapacity": 2,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "glue-encryption-job-1.py",
		"scriptLocation": "s3://aws-glue-assets-655214492809-ap-south-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-10-15T05:17:40.568Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-655214492809-ap-south-1/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-655214492809-ap-south-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "from awsglue.context import GlueContext\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.job import Job\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.sql.functions import col, lit, udf\r\nfrom pyspark.sql.types import StringType\r\nimport boto3\r\nimport botocore\r\nfrom boto3.dynamodb.conditions import Key, Attr\r\nimport hashlib\r\nimport sys\r\n\r\n# Required arguments\r\nargs = getResolvedOptions(sys.argv, ['JOB_NAME', 'tablename', 'sourcedate', 'batchid'])\r\n\r\n# Spark/Glue initialization\r\nsc = SparkContext()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\njob.init(args['JOB_NAME'], args)\r\n\r\n# DynamoDB: Get list of encrypted columns\r\nddb = boto3.resource('dynamodb')\r\ntable = ddb.Table('ddb-table-details')\r\n\r\nresponse = table.query(\r\n    KeyConditionExpression=Key('tablename').eq(args['tablename']),\r\n    FilterExpression=Attr('encryption').eq(True)\r\n)\r\n\r\nprint(f\"response from DynamoDB for tablename={args['tablename']}: {response}\")\r\ncolumns_to_hash = [item['columnname'] for item in response['Items']]\r\n\r\n# SHA-256 hashing function\r\ndef sha256_hash(v):\r\n    return hashlib.sha256(v.encode('utf-8')).hexdigest() if v else None\r\n\r\nhash_udf = udf(sha256_hash, StringType())\r\n\r\n# Read input data\r\ninput_path = f\"s3://aws-de-landing-data-bucket-gokul/{args['tablename']}/\"\r\ndf = spark.read.parquet(input_path)\r\nrecord_count = df.count()\r\nprint(f\"Input record count: {record_count}\")\r\n\r\nif record_count == 0:\r\n    print(\"Input dataset is empty. Exiting job gracefully.\")\r\n    job.commit()\r\n    sys.exit(0)\r\n\r\n# Ensure S3 buckets exist\r\nregion = \"ap-south-1\"\r\ns3 = boto3.client(\"s3\", region_name=region)\r\nfor bucket in ['masked-data-bucket-gokul', 'unmasked-data-bucket-gokul']:\r\n    try:\r\n        s3.head_bucket(Bucket=bucket)\r\n    except botocore.exceptions.ClientError as e:\r\n        error_code = int(e.response['Error']['Code'])\r\n        if error_code in [404, 403]:\r\n            s3.create_bucket(\r\n                Bucket=bucket,\r\n                CreateBucketConfiguration={'LocationConstraint': region}\r\n            )\r\nprint(\"S3 buckets verified/created.\")\r\n\r\n# Apply SHA-256 to sensitive columns\r\nmasked_df = df\r\nfor colname in columns_to_hash:\r\n    masked_df = masked_df.withColumn(f\"{colname}_sha256\", hash_udf(col(colname)))\r\n\r\n# Add partition columns\r\nmasked_df = masked_df.withColumn(\"tablename\", lit(args['tablename']))\r\nmasked_df = masked_df.withColumn(\"sourcedate\", lit(args['sourcedate']))\r\nmasked_df = masked_df.withColumn(\"batchid\", lit(args['batchid']))\r\n\r\n# Write masked data\r\nmasked_df.write.partitionBy(\"sourcedate\", \"batchid\") \\\r\n    .mode(\"overwrite\") \\\r\n    .parquet(f\"s3://masked-data-bucket-gokul/{args['tablename']}/\")\r\n\r\nprint(\"Masked data written.\")\r\n\r\n# Generate mapping of clear â†’ hash values\r\nmapping_rows = []\r\nfor colname in columns_to_hash:\r\n    unique_vals = df.select(colname).distinct().rdd.map(lambda row: row[0]).collect()\r\n    for val in unique_vals:\r\n        if val is not None:\r\n            mapping_rows.append((val, colname, args['tablename'], sha256_hash(val)))\r\n\r\n# Write mapping only if available\r\nif mapping_rows:\r\n    mapping_df = spark.createDataFrame(mapping_rows, [\"clear\", \"columnname\", \"tablename\", \"sha256_hash\"])\r\n    mapping_df.write.partitionBy(\"columnname\") \\\r\n        .mode(\"overwrite\") \\\r\n        .parquet(f\"s3://unmasked-data-bucket-gokul/{args['tablename']}/\")\r\n    print(\"Mapping data written.\")\r\nelse:\r\n    print(\"No mapping rows generated. Skipping writing unmasked mapping.\")\r\n\r\n\r\nimport boto3\r\nfrom botocore.exceptions import ClientError\r\n\r\nregion = \"ap-south-1\"\r\nglue_client = boto3.client(\"glue\", region_name=region)\r\n\r\ndatabase_name = \"masked_data_db\"\r\ntable_name = args[\"tablename\"]\r\ns3_table_path = f\"s3://masked-data-bucket-gokul/{table_name}/\"\r\n\r\n# 1. Create Glue Database if it doesn't exist\r\ntry:\r\n    glue_client.get_database(Name=database_name)\r\n    print(f\"Database '{database_name}' already exists.\")\r\nexcept glue_client.exceptions.EntityNotFoundException:\r\n    glue_client.create_database(DatabaseInput={\"Name\": database_name})\r\n    print(f\"Database '{database_name}' created.\")\r\n\r\n# 2. Check if table exists\r\ntry:\r\n    glue_client.get_table(DatabaseName=database_name, Name=table_name)\r\n    print(f\"Table '{table_name}' already exists in database '{database_name}'.\")\r\nexcept glue_client.exceptions.EntityNotFoundException:\r\n    print(f\"Table '{table_name}' not found. Creating from inferred schema...\")\r\n\r\n    # Infer schema from one of the Parquet partition paths\r\n    sample_partition_path = (\r\n        f\"s3://masked-data-bucket-gokul/{table_name}/sourcedate={args['sourcedate']}/batchid={args['batchid']}/\"\r\n    )\r\n    df_sample = spark.read.parquet(sample_partition_path)\r\n\r\n    # Set all columns to string type\r\n    glue_columns = [{\"Name\": field.name, \"Type\": \"string\"} for field in df_sample.schema.fields]\r\n\r\n    # Remove partition columns if present in data\r\n    glue_columns = [col for col in glue_columns if col[\"Name\"] not in [\"sourcedate\", \"batchid\"]]\r\n\r\n    glue_client.create_table(\r\n        DatabaseName=database_name,\r\n        TableInput={\r\n            \"Name\": table_name,\r\n            \"StorageDescriptor\": {\r\n                \"Columns\": glue_columns,\r\n                \"Location\": s3_table_path,\r\n                \"InputFormat\": \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\",\r\n                \"OutputFormat\": \"org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat\",\r\n                \"SerdeInfo\": {\r\n                    \"SerializationLibrary\": \"org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe\",\r\n                    \"Parameters\": {\"serialization.format\": \"1\"},\r\n                },\r\n            },\r\n            \"PartitionKeys\": [\r\n                {\"Name\": \"sourcedate\", \"Type\": \"string\"},\r\n                {\"Name\": \"batchid\", \"Type\": \"string\"},\r\n            ],\r\n            \"TableType\": \"EXTERNAL_TABLE\",\r\n            \"Parameters\": {\"classification\": \"parquet\", \"EXTERNAL\": \"TRUE\"},\r\n        },\r\n    )\r\n    print(f\"Glue table '{table_name}' created successfully.\")\r\n\r\njob.commit()\r\n"
}