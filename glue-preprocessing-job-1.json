{
	"jobConfig": {
		"name": "glue-preprocessing-job-1",
		"description": "",
		"role": "arn:aws:iam::655214492809:role/glue-IAM-Execution-role",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 2,
		"maxCapacity": 2,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "glue-preprocessing-job-1.py",
		"scriptLocation": "s3://aws-glue-assets-655214492809-ap-south-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-10-16T03:57:22.828Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-655214492809-ap-south-1/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-655214492809-ap-south-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "from awsglue.context import GlueContext\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.job import Job\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom pyspark.sql.functions import lit\r\nimport sys\r\nimport json\r\nimport boto3\r\n\r\n# Initialize with required parameters ONLY\r\nargs = getResolvedOptions(sys.argv, ['JOB_NAME', 'tablename', 's3_input', 'sourcedate', 'batchid'])\r\nprint(args['tablename'],args['s3_input'],args['sourcedate'],args['batchid'])\r\nsc = SparkContext()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\njob.init(args['JOB_NAME'], args)\r\n\r\n# Read data\r\ndf = spark.read.option(\"header\", \"true\").csv(args['s3_input'])\r\nprint(\"Data read successful\")\r\n\r\n# Clean columns\r\ndef clean(col):\r\n    return col.strip().lower().replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\r\n\r\nfor col in df.columns:\r\n    df = df.withColumnRenamed(col, clean(col))\r\n\r\nprint(\"column cleansed\")\r\n\r\n# Add metadata\r\ndf = df.withColumn(\"tablename\", lit(args['tablename'])) \\\r\n       .withColumn(\"sourcedate\", lit(args['sourcedate'])) \\\r\n       .withColumn(\"batchid\", lit(args['batchid']))\r\n\r\n# Write to HARDCODED bucket (or use parameter if available)\r\nbucket = 'aws-de-landing-data-bucket-gokul'  # Hardcoded or use args.get('output_bucket', 'default-bucket')\r\noutput_path = f\"s3://{bucket}/{args['tablename']}/\"\r\ndf.write.partitionBy(\"tablename\", \"sourcedate\", \"batchid\") \\\r\n       .mode(\"overwrite\").parquet(output_path)\r\n\r\nprint(\"Data write successful\")\r\n\r\nrecord_count = df.count()\r\nprint(f\"record_count: {record_count}\")\r\n\r\nrecord_count_output = {\"record_count\": record_count}\r\ns3_output_path = f\"s3://gokul-sandbox-test/record-count/{args['tablename']}_{args['sourcedate']}_{args['batchid']}.json\"\r\ns3 = boto3.client('s3')\r\ns3.put_object(Bucket='gokul-sandbox-test',\r\n              Key=f\"record-count/{args['tablename']}_{args['sourcedate']}_{args['batchid']}.json\",\r\n              Body=json.dumps(record_count_output))\r\n\r\n\r\njob.commit()"
}